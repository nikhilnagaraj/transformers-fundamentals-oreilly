{"cells":[{"cell_type":"markdown","metadata":{"id":"6KJoHTpcFd5n"},"source":["# Tutorial: Finetuning a BERT model for a Classification Task\n","\n","In this notebook, we will demonstrate how to fine-tune a BERT model on a text classification task using the Hugging Face Transformers library. We will be using the IMDB dataset as an example and the Bert-base model as our base model.\n","\n","Below you will find explanations, code snippets, and installation tips to help you follow along."]},{"cell_type":"markdown","metadata":{"id":"qETzXGg4Fd5r"},"source":["## Installation and Environment Setup\n","\n","Before starting, please ensure that you have the following libraries installed:\n","\n","- transformers\n","- datasets\n","- torch\n","\n","You can install them using pip:\n","\n","```bash\n","pip install transformers datasets torch\n","```\n","\n","> Note: This notebook is designed to be run on a pretty standard machine either on cpu or gpu. If you run into memory issues, consider using a smaller model or a managed cloud notebook environment."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ff4KXK1zFd5s","executionInfo":{"status":"ok","timestamp":1740910518293,"user_tz":-60,"elapsed":3317,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"bf140c60-c413-4131-8bda-0982a4a29030"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n","Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.3.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["! pip install transformers[torch] datasets torch"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"cO2sm_eRFd5u","executionInfo":{"status":"ok","timestamp":1740910536298,"user_tz":-60,"elapsed":4,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}}},"outputs":[],"source":["# Import the necessary libraries.\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from datasets import load_dataset\n","import numpy as np\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"TzSi3gyuFd5v"},"source":["## Dataset Preparation\n","\n","We will use the IMDB dataset, a standard benchmark for text classification. The dataset will be tokenized using the tokenizer corresponding to our base model. We also limit the maximum token length to 512 tokens."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c4563b023177492aa48d5ca40aa58db5","93761cda17924793815c17c753d2f193","adc85db867584c03a453d4da215cb423","53dc1d05050541538d7b362290bda43f","48dd7a7dc86649abb7a60600569ad305","f2d14f26fa1e45499566d77e9d96eb66","0cad78ab7c1f4491a624f520d91c6be3","9075875dee744c199cd1abbed59591d1","6d3ea69de7ad4eb6aa9a879dbb1d3e4d","ab389a02231146e08beadce95f14e66b","2a08aef6a1bf4e01a4196541faf9c1cd"]},"id":"Exfop3suFd5w","executionInfo":{"status":"ok","timestamp":1740910664292,"user_tz":-60,"elapsed":31827,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"74393d83-ddf5-4a11-dae9-217597a807e9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4563b023177492aa48d5ca40aa58db5"}},"metadata":{}}],"source":["# Load the IMDB dataset from the Hugging Face datasets library.\n","dataset = load_dataset(\"stanfordnlp/imdb\")\n","\n","# Define a function for tokenization.\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n","\n","# Load the tokenizer for our model.\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Tokenize the dataset.\n","tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"]},{"cell_type":"markdown","metadata":{"id":"0_xpFV_GFd5w"},"source":["## Model Setup\n","\n","We load the pre-trained Bert model and add a classification head. We set the number of output labels to 2 as the IMDB dataset is a binary classification task (positive/negative)."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85q2sgYkFd5x","executionInfo":{"status":"ok","timestamp":1740910688097,"user_tz":-60,"elapsed":169,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"dbfc3461-b70c-451d-839e-113937109a64"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Initialize the model with a classification head.\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"]},{"cell_type":"markdown","metadata":{"id":"Kp0pwtaiFd5x"},"source":["## Training and Evaluation Configuration\n","\n","Next, we define the training arguments. Here we set up evaluation strategy, batch sizes, number of epochs, and learning rate. We also define a helper function to compute accuracy during evaluation."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCLYHNAaFd5y","executionInfo":{"status":"ok","timestamp":1740910745919,"user_tz":-60,"elapsed":22,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"b4088200-cd4b-4770-ddf6-c91d2ff4451d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["# Configure the training arguments.\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=1,  # For demonstration; increase for better performance.\n","    weight_decay=0.01,\n","    push_to_hub=False,\n","    report_to=\"none\"\n",")\n","\n","# Define a metrics function to compute accuracy.\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = (predictions == labels).mean()\n","    return {\"accuracy\": accuracy}"]},{"cell_type":"markdown","metadata":{"id":"FiOiqD-4Fd5y"},"source":["## Setting Up the Trainer\n","\n","We now create a Trainer object from Hugging Face which manages the training loop, evaluation, and logging."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVaSIQX6Fd5y","executionInfo":{"status":"ok","timestamp":1740910783479,"user_tz":-60,"elapsed":300,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"af7290c9-efd0-48aa-fe62-b9e5e8ef6f45"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-30-99b8dade03c6>:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}],"source":["# Prepare a subset of the dataset for quick demonstration (use full dataset in practice).\n","train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n","eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n","\n","# Initialize the Trainer.\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"ha61ALTSFd5y","executionInfo":{"status":"ok","timestamp":1740910806916,"user_tz":-60,"elapsed":11219,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"00b10eea-947b-43c2-a15d-f23d28de2a37"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [125/125 00:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metrics before training: {'eval_loss': 0.7443788051605225, 'eval_model_preparation_time': 0.0059, 'eval_accuracy': 0.492, 'eval_runtime': 11.2058, 'eval_samples_per_second': 44.62, 'eval_steps_per_second': 11.155}\n"]}],"source":["## Evaluate the model before training.\n","metrics_before = trainer.evaluate(eval_dataset)\n","print(\"Metrics before training:\", metrics_before)"]},{"cell_type":"code","source":["import pandas as pd\n","from IPython.display import display, HTML\n","\n","samples = eval_dataset.select([0,20,40, 80, 100])\n","label_names = eval_dataset.features['label'].names\n","pred_out = trainer.predict(samples)\n","pred_labels = np.argmax(pred_out.predictions, axis=1)\n","true_labels = pred_out.label_ids\n","\n","pred_names = [label_names[i] for i in pred_labels]\n","true_names = [label_names[i] for i in true_labels]\n","inputs = [sample[\"text\"] for sample in samples]\n","\n","df_preds = pd.DataFrame({\n","    \"Input Text\": inputs,\n","    \"Prediction\": pred_names,\n","    \"True Label\": true_names\n","})\n","\n","display(HTML(df_preds.to_html(escape=False)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":918},"id":"0R9fhIjZRKxA","executionInfo":{"status":"ok","timestamp":1740910814535,"user_tz":-60,"elapsed":194,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"7779bff7-14ed-40d7-a791-e0f6c6a0f742"},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Input Text</th>\n","      <th>Prediction</th>\n","      <th>True Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td><br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br /></td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I was expecting a lot more of this film than what I actually got. The acting was just awful from everyone and the story was far from impressive. It took a lot of something I don't to even follow what was going because it was so jumpy. An example of the acting is when Paxton's character, Vann, is upset the South Vietnamese colonel for so he throws some of the sand from the \"sand map\". It was impossible to get any idea of what he was feeling and his actions were robotic. To make things worse, I have no idea how I'm supposed to feel about Vann. He's obviously presented as the protagonist but as soon as he gets to Vietnam he starts an affair with an Vietnamese English teacher. The only thing the movie had going for it was that it wasn't particularly boring. I give it 4 stars out of 10.</td>\n","      <td>pos</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>This film is bad. It's filled with glaring plot holes, characters who are ruled by stupidity, bad acting and above all, a poor script which has been done before in many, many films, only better. I feel sorry for Donald Sutherland, I just hope he had to do this film rather than wanted to! Miss it.</td>\n","      <td>pos</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Now and again, a film comes around purely by accident that makes you doubt your sanity. We just finished studying the novel, \"Northanger Abbey\", at school and decided to refresh our memory of this unexciting piece of humourless garbage with the BBC adaptation.<br /><br />The funny thing about Northanger Abbey is that it actually makes you want to kill yourself. The film is NOTHING like the book, for example, the subtly evil characters seem to have been turned into transparent stereotypes. John Thorpe looks like a leprechaun on acid while Isabella plays the role of slut. Catherine, the main character, is the most depressingly stupid and irritating actress on god's earth (she looks like a coffee addict, her eyes are like basketballs) whilst Mr Tilney looks and acts like a retired porno stunt double. The plot goes completely off the rails at certain points of the film, I don't know what the hell the director was thinking when for no reason at all, a 7 year old black kid who we've never met before takes the main character out of the abbey and starts cartwheeling in front of her. Yes, that's right, cartwheeling. Nonsense of this kind is occasionally interrupted by Catherines \"fantasies\" in which she is being carried around a cathedral by an ogre.<br /><br />Northanger Abbey is basically visual euthanasia so if you want to murder your boss or something like that, BBC have basically discovered a new way to kill someone. Northanger is a barely laughably bad film. Don't watch it unless you're in a padded cell.</td>\n","      <td>pos</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I wish I would have read more reviews and more opinions about this movie before I rented it. A waste of money. A waste of time. Very little dialog. The dialog was hard to understand in every way. The storyline and plot were both weak. The only thing that was nice at all was the cinematography.<br /><br />The characters were interesting. At the same time you will spend so much time trying to figure things out, because of the lack of dialog, that you will be rewinding the movie a lot. <br /><br />Do not watch this movie. It was a mess and will leave you feeling like a mess.<br /><br />You will say, what the heck was that, when the movie ends?</td>\n","      <td>pos</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"8K4rpOy5Fd5z"},"source":["## Finetuning the Model\n","\n","Now we start the fine-tuning process. Training a 1B parameter model can be resource intensive. For a thorough training, consider increasing the number of epochs and using gradient accumulation if needed."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"BLGmA2aiFd5z","executionInfo":{"status":"ok","timestamp":1740911084863,"user_tz":-60,"elapsed":201367,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"7ed4240e-2c86-4742-87e1-30a07f3c9a4f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 03:20, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Model Preparation Time</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.428500</td>\n","      <td>0.335327</td>\n","      <td>0.005900</td>\n","      <td>0.908000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=500, training_loss=0.42849285888671873, metrics={'train_runtime': 201.2436, 'train_samples_per_second': 9.938, 'train_steps_per_second': 2.485, 'total_flos': 431818686278880.0, 'train_loss': 0.42849285888671873, 'epoch': 1.0})"]},"metadata":{},"execution_count":33}],"source":["# Begin training\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"AzS2SNAgFd5z"},"source":["## Model Evaluation\n","\n","After training, evaluate the model on the evaluation dataset and print the performance metrics. Also check the predictions of the samples shown above."]},{"cell_type":"code","source":["samples = eval_dataset.select([0,20,40, 80, 100])\n","label_names = eval_dataset.features['label'].names\n","pred_out = trainer.predict(samples)\n","pred_labels = np.argmax(pred_out.predictions, axis=1)\n","true_labels = pred_out.label_ids\n","\n","pred_names = [label_names[i] for i in pred_labels]\n","true_names = [label_names[i] for i in true_labels]\n","inputs = [sample[\"text\"] for sample in samples]\n","\n","df_preds = pd.DataFrame({\n","    \"Input Text\": inputs,\n","    \"Prediction\": pred_names,\n","    \"True Label\": true_names\n","})\n","\n","display(HTML(df_preds.to_html(escape=False)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":918},"id":"8R1R_rvMUK73","executionInfo":{"status":"ok","timestamp":1740911107007,"user_tz":-60,"elapsed":197,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"cd0eb3b0-eb67-456c-f2be-2022b09ad98d"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Input Text</th>\n","      <th>Prediction</th>\n","      <th>True Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td><br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br /></td>\n","      <td>pos</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I was expecting a lot more of this film than what I actually got. The acting was just awful from everyone and the story was far from impressive. It took a lot of something I don't to even follow what was going because it was so jumpy. An example of the acting is when Paxton's character, Vann, is upset the South Vietnamese colonel for so he throws some of the sand from the \"sand map\". It was impossible to get any idea of what he was feeling and his actions were robotic. To make things worse, I have no idea how I'm supposed to feel about Vann. He's obviously presented as the protagonist but as soon as he gets to Vietnam he starts an affair with an Vietnamese English teacher. The only thing the movie had going for it was that it wasn't particularly boring. I give it 4 stars out of 10.</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>This film is bad. It's filled with glaring plot holes, characters who are ruled by stupidity, bad acting and above all, a poor script which has been done before in many, many films, only better. I feel sorry for Donald Sutherland, I just hope he had to do this film rather than wanted to! Miss it.</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Now and again, a film comes around purely by accident that makes you doubt your sanity. We just finished studying the novel, \"Northanger Abbey\", at school and decided to refresh our memory of this unexciting piece of humourless garbage with the BBC adaptation.<br /><br />The funny thing about Northanger Abbey is that it actually makes you want to kill yourself. The film is NOTHING like the book, for example, the subtly evil characters seem to have been turned into transparent stereotypes. John Thorpe looks like a leprechaun on acid while Isabella plays the role of slut. Catherine, the main character, is the most depressingly stupid and irritating actress on god's earth (she looks like a coffee addict, her eyes are like basketballs) whilst Mr Tilney looks and acts like a retired porno stunt double. The plot goes completely off the rails at certain points of the film, I don't know what the hell the director was thinking when for no reason at all, a 7 year old black kid who we've never met before takes the main character out of the abbey and starts cartwheeling in front of her. Yes, that's right, cartwheeling. Nonsense of this kind is occasionally interrupted by Catherines \"fantasies\" in which she is being carried around a cathedral by an ogre.<br /><br />Northanger Abbey is basically visual euthanasia so if you want to murder your boss or something like that, BBC have basically discovered a new way to kill someone. Northanger is a barely laughably bad film. Don't watch it unless you're in a padded cell.</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I wish I would have read more reviews and more opinions about this movie before I rented it. A waste of money. A waste of time. Very little dialog. The dialog was hard to understand in every way. The storyline and plot were both weak. The only thing that was nice at all was the cinematography.<br /><br />The characters were interesting. At the same time you will spend so much time trying to figure things out, because of the lack of dialog, that you will be rewinding the movie a lot. <br /><br />Do not watch this movie. It was a mess and will leave you feeling like a mess.<br /><br />You will say, what the heck was that, when the movie ends?</td>\n","      <td>neg</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"eQ6tIVGyFd5z","executionInfo":{"status":"ok","timestamp":1740911140602,"user_tz":-60,"elapsed":12930,"user":{"displayName":"Nikhil Nagaraj","userId":"17133742275820801063"}},"outputId":"a2824ce5-2d34-47d3-edd4-60d988db545d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [125/125 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["                 eval_loss  eval_model_preparation_time  eval_accuracy  \\\n","Before Training   0.744379                       0.0059          0.492   \n","After Training    0.335327                       0.0059          0.908   \n","\n","                 eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n","Before Training       11.2058                   44.620                 11.155   \n","After Training        12.8943                   38.777                  9.694   \n","\n","                 epoch  \n","Before Training    NaN  \n","After Training     1.0  "],"text/html":["\n","  <div id=\"df-610fa425-7dba-464e-bafe-2fca255da3a0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eval_loss</th>\n","      <th>eval_model_preparation_time</th>\n","      <th>eval_accuracy</th>\n","      <th>eval_runtime</th>\n","      <th>eval_samples_per_second</th>\n","      <th>eval_steps_per_second</th>\n","      <th>epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Before Training</th>\n","      <td>0.744379</td>\n","      <td>0.0059</td>\n","      <td>0.492</td>\n","      <td>11.2058</td>\n","      <td>44.620</td>\n","      <td>11.155</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>After Training</th>\n","      <td>0.335327</td>\n","      <td>0.0059</td>\n","      <td>0.908</td>\n","      <td>12.8943</td>\n","      <td>38.777</td>\n","      <td>9.694</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-610fa425-7dba-464e-bafe-2fca255da3a0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-610fa425-7dba-464e-bafe-2fca255da3a0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-610fa425-7dba-464e-bafe-2fca255da3a0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4150351c-d799-4cc9-8a8f-7a50f70279fa\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4150351c-d799-4cc9-8a8f-7a50f70279fa')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4150351c-d799-4cc9-8a8f-7a50f70279fa button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_5e0796d6-bf27-42df-8bf0-87a743591d7e\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_table')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_5e0796d6-bf27-42df-8bf0-87a743591d7e button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('metrics_table');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"metrics_table","summary":"{\n  \"name\": \"metrics_table\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"eval_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2892435374992291,\n        \"min\": 0.3353266716003418,\n        \"max\": 0.7443788051605225,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3353266716003418,\n          0.7443788051605225\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_model_preparation_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0059,\n        \"max\": 0.0059,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0059\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2941564209736038,\n        \"min\": 0.492,\n        \"max\": 0.908,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.908\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_runtime\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1939498000334852,\n        \"min\": 11.2058,\n        \"max\": 12.8943,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          12.8943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_samples_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.131624922472994,\n        \"min\": 38.777,\n        \"max\": 44.62,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          38.777\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_steps_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0330830073135449,\n        \"min\": 9.694,\n        \"max\": 11.155,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9.694\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":35}],"source":["# Evaluate the fine-tuned model.\n","metrics_after = trainer.evaluate()\n","metrics_table = pd.DataFrame([metrics_before, metrics_after],\n","                               index=[\"Before Training\", \"After Training\"])\n","metrics_table"]},{"cell_type":"markdown","metadata":{"id":"0Wu4dODuFd50"},"source":["## Final Notes\n","\n","- For production-level fine-tuning, consider using distributed training and mixed precision (fp16).\n","- Experiment with hyperparameters such as learning rate and batch size to optimize performance.\n","- For further improvements, consider using the Hugging Face Hub for version control and model sharing."]},{"cell_type":"markdown","metadata":{"id":"hJxqUx-bFd50"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c4563b023177492aa48d5ca40aa58db5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93761cda17924793815c17c753d2f193","IPY_MODEL_adc85db867584c03a453d4da215cb423","IPY_MODEL_53dc1d05050541538d7b362290bda43f"],"layout":"IPY_MODEL_48dd7a7dc86649abb7a60600569ad305"}},"93761cda17924793815c17c753d2f193":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2d14f26fa1e45499566d77e9d96eb66","placeholder":"â€‹","style":"IPY_MODEL_0cad78ab7c1f4491a624f520d91c6be3","value":"Map:â€‡100%"}},"adc85db867584c03a453d4da215cb423":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9075875dee744c199cd1abbed59591d1","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d3ea69de7ad4eb6aa9a879dbb1d3e4d","value":25000}},"53dc1d05050541538d7b362290bda43f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab389a02231146e08beadce95f14e66b","placeholder":"â€‹","style":"IPY_MODEL_2a08aef6a1bf4e01a4196541faf9c1cd","value":"â€‡25000/25000â€‡[00:29&lt;00:00,â€‡1191.45â€‡examples/s]"}},"48dd7a7dc86649abb7a60600569ad305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d14f26fa1e45499566d77e9d96eb66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cad78ab7c1f4491a624f520d91c6be3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9075875dee744c199cd1abbed59591d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3ea69de7ad4eb6aa9a879dbb1d3e4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab389a02231146e08beadce95f14e66b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a08aef6a1bf4e01a4196541faf9c1cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}